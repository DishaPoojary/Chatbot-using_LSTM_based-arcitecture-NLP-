# Chatbot-using_LSTM_based-arcitecture-NLP-

The code begins by importing necessary libraries and loading the training and test data from pickle files. It then processes the data to create a vocabulary set by extracting unique words from the stories and questions. The vocabulary set is further extended with the addition of 'yes' and 'no' words. The code vectorizes the training data using a tokenizer and creates sequences for the stories and questions. It also converts the answers into one-hot encoded format. The model architecture is defined, consisting of input encoders for the story and question, and an LSTM-based model for generating the answer. The encoded inputs are passed through various layers, including embeddings, dropout, and concatenation. The model is compiled and trained using the training data. The code includes a visualization of the model's training history using matplotlib. The trained model is saved and later loaded for evaluation on the test set. The code performs predictions on specific test samples and provides the predicted answer and the associated probability. Finally, the code demonstrates how to use the trained model to answer custom questions by creating a new story and question, vectorizing them, and making predictions.
LSTM stands for Long Short-Term Memory, which is a type of recurrent neural network (RNN) architecture. LSTM networks are designed to overcome the limitations of traditional RNNs in capturing long-term dependencies in sequential data.LSTM networks are particularly effective in processing sequential data, such as text, speech, and time series data, where long-range dependencies are important. They have been widely used in various natural language processing (NLP) tasks, including language translation, sentiment analysis, and question answering, among others.
